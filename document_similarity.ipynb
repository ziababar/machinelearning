{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document-similarity.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziababar/machinelearning/blob/master/document_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBEUj0wzWt7X",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "\n",
        "\n",
        "\n",
        "Objective\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLES_4eKGyjp",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFhxliRRPm4",
        "colab_type": "text"
      },
      "source": [
        "**Natural language toolkit (NLTK)** is the most popular library for natural language processing (NLP) which was written in Python and has a big community behind it. NLTK also is very easy to learn, actually, it’ s the easiest natural language processing (NLP) library that we are going to use. It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
        "\n",
        "**Gensim** is billed as a Natural Language Processing package that does ‘Topic Modeling for Humans’. But it is practically much more than that. It is a leading and a state-of-the-art package for processing texts, working with word vector models (such as Word2Vec, FastText etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-FnFlOEROOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install nltk and gensim by following commands\n",
        "import gensim\n",
        "import nltk\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LrFc_PDRUCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
        "from nltk.tokenize import sent_tokenize # Sentence Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI79I4YHRZVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(dir(gensim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScKOVn2-G8_E",
        "colab_type": "text"
      },
      "source": [
        "# Data Sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greBaHqGRfw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_documents = [\"Someone I know recently combined Maple Syrup & buttered Popcorn thinking it would taste like caramel popcorn. It didn’t and they don’t recommend anyone else do it either.\",\n",
        "                 \"Sometimes it is better to just walk away from things and go back to them later when you’re in a better frame of mind.\",\n",
        "                 \"Italy is my favorite country; in fact, I plan to spend two weeks there next year.\",\n",
        "                 \"He turned in the research paper on Friday; otherwise, he would have not passed the class.\",\n",
        "                 \"Keep up with evolving privacy and security regulations across all industries with monitoring and detailed reporting from encryption status to failed authentication and everything in between. \"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lapzvT1ORni0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"Number of documents:\",len(raw_documents))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4pXI8GHHb7P",
        "colab_type": "text"
      },
      "source": [
        "# Document Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBg1Mdp3Rp_G",
        "colab_type": "text"
      },
      "source": [
        "**Open document 1**\n",
        "\n",
        "**Open document 2**\n",
        "\n",
        "**Open document 3**\n",
        "\n",
        "\n",
        "need to review them too"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZZ8NC6YIHyj",
        "colab_type": "text"
      },
      "source": [
        "Create a .txt file and write 4-5 sentences in it. Include the file with the same directory of your Python program. Now, we are going to open this file with Python and split sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJhNFapNIDzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_docs = []\n",
        "\n",
        "with open ('demofile.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHZYizEHgJY",
        "colab_type": "text"
      },
      "source": [
        "### Method 1 - Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-26OkEAFRufF",
        "colab_type": "text"
      },
      "source": [
        "We use the method word_tokenize() to split a sentence into words. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbzjw93DIPYa",
        "colab_type": "text"
      },
      "source": [
        "Once we added tokenized sentences in array, it is time to tokenize words for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn3Hm6MHRjMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
        "            for text in raw_documents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLN-U2NuRziU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(gen_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTVTaV3mRrcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "#print(dictionary[5])\n",
        "#print(dictionary.token2id['road'])\n",
        "#print(\"Number of words in dictionary:\",len(dictionary))\n",
        "#for i in range(len(dictionary)):\n",
        "#    print(i, dictionary[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POqBiZMUSkVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "#print(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyiKRpSaIask",
        "colab_type": "text"
      },
      "source": [
        "### Method 2 - TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxDawqPGSoMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "#print(tf_idf)\n",
        "s = 0\n",
        "for i in corpus:\n",
        "    s += len(i)\n",
        "#print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49GcziAzIflb",
        "colab_type": "text"
      },
      "source": [
        "# Determining Document Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT2f_klUInRF",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to create similarity object. The main class is Similarity, which builds an index for a given set of documents.The Similarity class splits the index into several smaller sub-indexes, which are disk-based. Let's just create similarity object then you will understand how we can use it for comparing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKAOPsWSqs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sims = gensim.similarities.Similarity('/usr',tf_idf[corpus], num_features=len(dictionary))\n",
        "#print(sims)\n",
        "#print(type(sims))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN5V3fojItQM",
        "colab_type": "text"
      },
      "source": [
        "**Create Query Document**\n",
        "\n",
        "Once the index is built, we are going to calculate how similar is this query document to each document in the index. So, create second .txt file which will include query documents or sentences and tokenize them as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiNJ9UQCSrgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_doc_1 = [w.lower() for w in word_tokenize(\"Evolving privacy and security regulations.\")]\n",
        "query_doc_bow = dictionary.doc2bow(query_doc_1)\n",
        "\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sims[query_doc_tf_idf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPBfbDhJIyo",
        "colab_type": "text"
      },
      "source": [
        "Once the index is built, we are going to calculate how similar is this query document to each document in the index. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvtllxKJKSO",
        "colab_type": "text"
      },
      "source": [
        "**Document similarities to query**\n",
        "\n",
        "At this stage, you will see similarities between the query and all index documents. To obtain similarities of our query document against the indexed documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaNERDHZTXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_doc_2 = [w.lower() for w in word_tokenize(\"Data encrytion is important for enterprises looking for data security and data privacy.\")]\n",
        "query_doc_bow = dictionary.doc2bow(query_doc_2)\n",
        "\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sims[query_doc_tf_idf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WIt4SP_Svta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_doc_3 = [w.lower() for w in word_tokenize(\"He turned in the research paper on Friday; otherwise, he would have not passed the class.\")]\n",
        "query_doc_bow = dictionary.doc2bow(query_doc_3)\n",
        "\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "sims[query_doc_tf_idf]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydVUueBBKP0r",
        "colab_type": "text"
      },
      "source": [
        "**Computing Cosine Similarity using scikit-learn**\n",
        "\n",
        "Generally a cosine similarity between two documents is used as a similarity measure of documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX4db1K9Kfat",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKQo9wNxKOrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [open(f) for f in text_files]\n",
        "tfidf = TfidfVectorizer().fit_transform(documents)\n",
        "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
        "pairwise_similarity = tfidf * tfidf.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpd9aRAjKZV9",
        "colab_type": "text"
      },
      "source": [
        ">>> corpus = [\"I'd like an apple\", \n",
        "...           \"An apple a day keeps the doctor away\", \n",
        "...           \"Never compare an apple to an orange\", \n",
        "...           \"I prefer scikit-learn to Orange\", \n",
        "...           \"The scikit-learn docs are Orange and Blue\"]        \n",
        ">>> vect = TfidfVectorizer(min_df=1, stop_words=\"english\")    >>> tfidf = vect.fit_transform(corpus)                        >>> pairwise_similarity = tfidf * tfidf.T "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-z9cTeK6fW",
        "colab_type": "text"
      },
      "source": [
        "**DOT PRODUCT** method"
      ]
    }
  ]
}